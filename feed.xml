<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://gauthiercler.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gauthiercler.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-27T19:45:31+00:00</updated><id>https://gauthiercler.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Adversarial Attacks</title><link href="https://gauthiercler.github.io/blog/2021/adversarial-attacks/" rel="alternate" type="text/html" title="Adversarial Attacks"/><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://gauthiercler.github.io/blog/2021/adversarial-attacks</id><content type="html" xml:base="https://gauthiercler.github.io/blog/2021/adversarial-attacks/"><![CDATA[<p>Adversarial attacks on machine learning is a wide domain of interest. In this first, we cover white box adversarial attacks using the gradient sign method. In a white box setting, the attacker has access to all the information used for training the model, its weights and parameters, along the used datasets for training.</p> <h3 id="fgsm">FGSM</h3> <p>The Fast Gradient Sign Method (FGSM) is a straightforward approach proposed in <d-cite key="goodfellow2015explainingharnessingadversarialexamples"></d-cite> to generate adversarial sample from a trained model. Given the model parameters $\boldsymbol{\theta}$, some input samples $\boldsymbol{x}$ and associated ground truth label $y$, the general objective that the model aims at minimizing during training is defined as:</p> \[J(\boldsymbol{\theta}, \boldsymbol{x}, y)\] <p>The adversarial perturbation is given as:</p> \[\boldsymbol{\eta} = \epsilon \text{sign}(\nabla_{\boldsymbol{x}} J(\boldsymbol{\theta}, \boldsymbol{x}, y)).\] <p>By taking the gradient of the loss w.r.t to $\boldsymbol{x}$, it provides the direction in which the input $\boldsymbol{x}$ should change in order to maximize the loss. Thus, a new generated adversarial sample can simply be generated as $\boldsymbol{x’} = \boldsymbol{x} + \boldsymbol{\eta}$. Given a small value of $\epsilon$, a new generated sample cannot be distinguished by the human eye from a real one, but is sufficient to deceive the model. In other words, a slight alteration of the input image pixels causes a consequent change in the model embedding.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fgsm-480.webp 480w,/assets/img/fgsm-800.webp 800w,/assets/img/fgsm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fgsm.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Image from <d-cite key="goodfellow2015explainingharnessingadversarialexamples"></d-cite>.</figcaption> </figure> <p>However, this approach only causes the model to missclassify samples. Instead, an variant can be used to target a specific class to be predicted by the model, maximizing $p(\hat{y}|\boldsymbol{x})$, where $\hat{y}$ corresponds to the label of the targeted class. Previous perturbation now becomes:</p> \[\boldsymbol{\eta} = \epsilon \text{sign}(\nabla_{\boldsymbol{x}} J(\boldsymbol{\theta}, \boldsymbol{x}, \hat{y})).\] <p>This time, as we aim at going in the direction that minimize the loss (instead of maximizing it) of the target class $\hat{y}$, the adversarial sample becomes $\boldsymbol{x’} = \boldsymbol{x} - \boldsymbol{\eta}$. The choice of the target class can be motivated by a specific attack strategy (for example predict dogs as cats). In <d-cite key="kurakin2017adversarialexamplesphysicalworld"></d-cite>, authors suggest to choose the least likely class based on the trained model prediction:</p> \[\hat{y} = \underset{y}{\arg\min} \{p(y|\boldsymbol{x})\}\] <p>This is motivated by the fact that least likely predicted class in is higly dissimilar to the actual true class. This can be used to maximize the model misclassification.</p> <p>This adversarial process can also be applied iteratively with a small step size.</p> \[\boldsymbol{x'}_{n + 1} = \boldsymbol{x'}_{n} - \epsilon \text{sign}(\nabla_{\boldsymbol{x}_n} J(\boldsymbol{\theta}, \boldsymbol{x}, \hat{y}))\] <aside><p>Note that clipping may be necessary depending on the data used.</p></aside> <p>with $\boldsymbol{x’}_{0} = \boldsymbol{x}$. This latter approach is more effective than the former one step generation as it produce smaller perturbations and is less destructive.</p> <h3 id="adversarial-training">Adversarial training</h3> <p>This adversarial generation method can be used as a regularization for training more robust models. In its simplest way, it can be formulated as:</p> \[\tilde{J}(\boldsymbol{\theta}, \boldsymbol{x}, y) = \alpha J(\boldsymbol{\theta}, \boldsymbol{x}, y) + (1 - \alpha) \epsilon \text{sign}(\nabla_{\boldsymbol{x}} J(\boldsymbol{\theta}, \boldsymbol{x}, y)).\]]]></content><author><name></name></author><category term="adversarial"/><summary type="html"><![CDATA[Adversarial attacks on machine learning is a wide domain of interest. In this first, we cover white box adversarial attacks using the gradient sign method. In a white box setting, the attacker has access to all the information used for training the model, its weights and parameters, along the used datasets for training.]]></summary></entry></feed>